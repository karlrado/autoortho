#!/usr/bin/env python3
"""
bench_pipeline.py - Performance benchmarks for native pipeline

Compares native vs Python implementations for:
- Cache file reading
- JPEG decoding
- DDS building (full pipeline)
- Complete tile generation
- END-TO-END tile workflow (creation → cache → DDS → serve)

Measures the REAL speedup from native C+OpenMP implementation vs pure Python.

Usage:
    PYTHONPATH=/path/to/autoortho4xplane python autoortho/tests/bench_pipeline.py

Or run directly:
    python -m autoortho.tests.bench_pipeline
"""

import os
import sys
import time
import tempfile
from io import BytesIO
from typing import List, Tuple, Optional

# Add parent to path for imports
_script_dir = os.path.dirname(os.path.abspath(__file__))
_autoortho_dir = os.path.dirname(_script_dir)
_project_dir = os.path.dirname(_autoortho_dir)
if _project_dir not in sys.path:
    sys.path.insert(0, _project_dir)


# ============================================================================
# Test Data Generation
# ============================================================================

def create_realistic_jpeg(width: int = 256, height: int = 256,
                          quality: int = 85) -> bytes:
    """Create a realistic JPEG with actual image content."""
    # First, try to use the actual test file from the repo
    test_file = os.path.join(_autoortho_dir, 'testfiles', 'test_tile_small.jpg')
    if os.path.exists(test_file):
        with open(test_file, 'rb') as f:
            return f.read()

    try:
        # Fallback to PIL
        from PIL import Image
        import random
        random.seed(42)

        img = Image.new('RGB', (width, height))
        pixels = img.load()
        for y in range(height):
            for x in range(width):
                r = int((x / width) * 128 + 64) + random.randint(-20, 20)
                g = int((y / height) * 100 + 100) + random.randint(-20, 20)
                b = int(((x+y) / (width+height)) * 100 + 50) + random.randint(-20, 20)
                pixels[x, y] = (
                    min(255, max(0, r)),
                    min(255, max(0, g)),
                    min(255, max(0, b))
                )

        buf = BytesIO()
        img.save(buf, format='JPEG', quality=quality)
        return buf.getvalue()
    except ImportError:
        pass

    # Ultimate fallback: minimal valid JPEG
    return create_minimal_jpeg()


def create_minimal_jpeg() -> bytes:
    """Create a minimal valid JPEG for basic testing."""
    return bytes([
        0xFF, 0xD8, 0xFF, 0xE0, 0x00, 0x10, 0x4A, 0x46, 0x49, 0x46, 0x00, 0x01,
        0x01, 0x00, 0x00, 0x01, 0x00, 0x01, 0x00, 0x00, 0xFF, 0xDB, 0x00, 0x43,
        0x00, 0x08, 0x06, 0x06, 0x07, 0x06, 0x05, 0x08, 0x07, 0x07, 0x07, 0x09,
        0x09, 0x08, 0x0A, 0x0C, 0x14, 0x0D, 0x0C, 0x0B, 0x0B, 0x0C, 0x19, 0x12,
        0x13, 0x0F, 0x14, 0x1D, 0x1A, 0x1F, 0x1E, 0x1D, 0x1A, 0x1C, 0x1C, 0x20,
        0x24, 0x2E, 0x27, 0x20, 0x22, 0x2C, 0x23, 0x1C, 0x1C, 0x28, 0x37, 0x29,
        0x2C, 0x30, 0x31, 0x34, 0x34, 0x34, 0x1F, 0x27, 0x39, 0x3D, 0x38, 0x32,
        0x3C, 0x2E, 0x33, 0x34, 0x32, 0xFF, 0xC0, 0x00, 0x0B, 0x08, 0x00, 0x01,
        0x00, 0x01, 0x01, 0x01, 0x11, 0x00, 0xFF, 0xC4, 0x00, 0x1F, 0x00, 0x00,
        0x01, 0x05, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08,
        0x09, 0x0A, 0x0B, 0xFF, 0xC4, 0x00, 0xB5, 0x10, 0x00, 0x02, 0x01, 0x03,
        0x03, 0x02, 0x04, 0x03, 0x05, 0x05, 0x04, 0x04, 0x00, 0x00, 0x01, 0x7D,
        0x01, 0x02, 0x03, 0x00, 0x04, 0x11, 0x05, 0x12, 0x21, 0x31, 0x41, 0x06,
        0x13, 0x51, 0x61, 0x07, 0x22, 0x71, 0x14, 0x32, 0x81, 0x91, 0xA1, 0x08,
        0x23, 0x42, 0xB1, 0xC1, 0x15, 0x52, 0xD1, 0xF0, 0x24, 0x33, 0x62, 0x72,
        0x82, 0x09, 0x0A, 0x16, 0x17, 0x18, 0x19, 0x1A, 0x25, 0x26, 0x27, 0x28,
        0x29, 0x2A, 0x34, 0x35, 0x36, 0x37, 0x38, 0x39, 0x3A, 0x43, 0x44, 0x45,
        0x46, 0x47, 0x48, 0x49, 0x4A, 0x53, 0x54, 0x55, 0x56, 0x57, 0x58, 0x59,
        0x5A, 0x63, 0x64, 0x65, 0x66, 0x67, 0x68, 0x69, 0x6A, 0x73, 0x74, 0x75,
        0x76, 0x77, 0x78, 0x79, 0x7A, 0x83, 0x84, 0x85, 0x86, 0x87, 0x88, 0x89,
        0x8A, 0x92, 0x93, 0x94, 0x95, 0x96, 0x97, 0x98, 0x99, 0x9A, 0xA2, 0xA3,
        0xA4, 0xA5, 0xA6, 0xA7, 0xA8, 0xA9, 0xAA, 0xB2, 0xB3, 0xB4, 0xB5, 0xB6,
        0xB7, 0xB8, 0xB9, 0xBA, 0xC2, 0xC3, 0xC4, 0xC5, 0xC6, 0xC7, 0xC8, 0xC9,
        0xCA, 0xD2, 0xD3, 0xD4, 0xD5, 0xD6, 0xD7, 0xD8, 0xD9, 0xDA, 0xE1, 0xE2,
        0xE3, 0xE4, 0xE5, 0xE6, 0xE7, 0xE8, 0xE9, 0xEA, 0xF1, 0xF2, 0xF3, 0xF4,
        0xF5, 0xF6, 0xF7, 0xF8, 0xF9, 0xFA, 0xFF, 0xDA, 0x00, 0x08, 0x01, 0x01,
        0x00, 0x00, 0x3F, 0x00, 0xFB, 0xD2, 0x8A, 0x28, 0x03, 0xFF, 0xD9
    ])


def create_test_cache_dir(num_chunks: int = 256, use_realistic: bool = True) -> Tuple[str, List[str]]:
    """Create a temporary cache directory with test JPEG files."""
    tmpdir = tempfile.mkdtemp(prefix="aopipeline_bench_")
    
    print(f"  Creating {num_chunks} test JPEG files...")
    if use_realistic:
        jpeg_data = create_realistic_jpeg(256, 256, quality=85)
        print(f"  JPEG size: {len(jpeg_data)} bytes (realistic)")
    else:
        jpeg_data = create_minimal_jpeg()
        print(f"  JPEG size: {len(jpeg_data)} bytes (minimal)")
    
    paths = []
    chunks_per_side = int(num_chunks ** 0.5)
    for i in range(num_chunks):
        row = i // chunks_per_side
        col = i % chunks_per_side
        path = os.path.join(tmpdir, f"{col}_{row}_16_BI.jpg")
        with open(path, 'wb') as f:
            f.write(jpeg_data)
        paths.append(path)
    
    return tmpdir, paths


def cleanup_test_cache(cache_dir: str):
    """Remove test cache directory."""
    import shutil
    try:
        shutil.rmtree(cache_dir)
    except OSError:
        pass


# ============================================================================
# Python Baseline Implementations
# ============================================================================

def python_sequential_read(paths: List[str]) -> List[bytes]:
    """Python baseline: sequential file reads."""
    results = []
    for path in paths:
        try:
            with open(path, 'rb') as f:
                results.append(f.read())
        except (FileNotFoundError, OSError):
            results.append(b'')
    return results


def python_batch_read(paths: List[str]) -> List[bytes]:
    """Python baseline: file reads (still sequential due to GIL)."""
    return python_sequential_read(paths)


def python_decode_jpegs(jpeg_datas: List[bytes]) -> List[Optional[bytes]]:
    """Python baseline: decode JPEGs using AoImage or PIL."""
    try:
        from autoortho.aoimage import AoImage as aoimage_module
        results = []
        for jpeg_data in jpeg_datas:
            if jpeg_data:
                try:
                    # AoImage uses module-level load_from_memory function
                    img = aoimage_module.load_from_memory(jpeg_data)
                    if img and img._width > 0:
                        results.append(img.data_ptr())
                    else:
                        results.append(None)
                except Exception:
                    results.append(None)
            else:
                results.append(None)
        return results
    except ImportError:
        pass

    try:
        from PIL import Image
        results = []
        for jpeg_data in jpeg_datas:
            if jpeg_data:
                try:
                    img = Image.open(BytesIO(jpeg_data))
                    img = img.convert('RGBA')
                    results.append(img.tobytes())
                except Exception:
                    results.append(None)
            else:
                results.append(None)
        return results
    except ImportError:
        return [None] * len(jpeg_datas)


def python_build_dds_full(
    cache_dir: str, chunks_per_side: int = 4
) -> Optional[bytes]:
    """
    Python baseline: Full DDS generation pipeline.

    This replicates what the native pipeline does:
    1. Read all JPEG cache files
    2. Decode each JPEG to RGBA
    3. Compose into full tile
    4. Generate mipmaps
    5. Compress with DXT/BC1
    """
    try:
        from autoortho.aoimage import AoImage as aoimage_module
        from autoortho.pydds import DDS
    except ImportError as e:
        print(f"  Python DDS build not available: {e}")
        return None

    chunk_size = 256
    tile_size = chunks_per_side * chunk_size

    # Step 1: Read all cache files
    paths = []
    for row in range(chunks_per_side):
        for col in range(chunks_per_side):
            path = os.path.join(cache_dir, f"{col}_{row}_16_BI.jpg")
            paths.append(path)

    jpeg_datas = python_sequential_read(paths)

    # Step 2: Decode JPEGs
    images = []
    for jpeg_data in jpeg_datas:
        if jpeg_data:
            try:
                img = aoimage_module.load_from_memory(jpeg_data)
                images.append(img)
            except Exception:
                images.append(None)
        else:
            images.append(None)

    # Step 3: Compose into full tile image
    # Create a new image for the tile using module function
    try:
        tile_img = aoimage_module.new('RGBA', (tile_size, tile_size), (0, 0, 0))
        for i, img in enumerate(images):
            if img is not None and img._width > 0:
                row = i // chunks_per_side
                col = i % chunks_per_side
                x = col * chunk_size
                y = row * chunk_size
                tile_img.paste(img, (x, y))
    except Exception as e:
        # Fallback: just create a blank tile
        print(f"  Compose failed: {e}")
        tile_img = aoimage_module.new('RGBA', (tile_size, tile_size), (0, 0, 0))

    # Step 4 & 5: Generate DDS with mipmaps and compression
    dds = DDS(tile_size, tile_size, ispc=True, dxt_format="BC1")
    dds.gen_mipmaps(tile_img, startmipmap=0, maxmipmaps=8)

    # Collect the output
    output = BytesIO()
    output.write(dds.header.getvalue())
    for mipmap in dds.mipmap_list:
        if mipmap.databuffer is not None:
            output.write(mipmap.databuffer.getvalue())

    return output.getvalue()


# ============================================================================
# Benchmark Functions
# ============================================================================

def benchmark_cache_read(num_files: int = 256, iterations: int = 5):
    """Benchmark cache reading: native vs Python."""
    print(f"\n{'='*70}")
    print(f"CACHE READ BENCHMARK ({num_files} files, {iterations} iterations)")
    print(f"{'='*70}")
    
    # Create test data with realistic JPEG content
    cache_dir, paths = create_test_cache_dir(num_files, use_realistic=True)
    
    try:
        # Python baseline
        print("\n  Running Python sequential read...")
        python_times = []
        for i in range(iterations):
            start = time.perf_counter()
            results = python_sequential_read(paths)
            python_times.append(time.perf_counter() - start)
        
        python_avg = sum(python_times) / len(python_times) * 1000
        total_bytes = sum(len(r) for r in results)
        print(f"  Python sequential: {python_avg:.2f}ms avg ({total_bytes/1024:.0f} KB)")
        
        # Native (if available)
        try:
            from autoortho.aopipeline import AoCache
            if AoCache.is_available():
                print(f"  Running Native parallel read...")
                # Warmup
                AoCache.batch_read_cache(paths[:10])
                
                native_times = []
                for i in range(iterations):
                    start = time.perf_counter()
                    results = AoCache.batch_read_cache(paths)
                    native_times.append(time.perf_counter() - start)
                
                native_avg = sum(native_times) / len(native_times) * 1000
                speedup = python_avg / native_avg if native_avg > 0 else float('inf')
                
                print(f"\n  Results:")
                print(f"    Python sequential: {python_avg:.2f}ms")
                print(f"    Native parallel:   {native_avg:.2f}ms")
                print(f"    Speedup:           {speedup:.1f}x")
            else:
                print("  Native: not available")
        except ImportError as e:
            print(f"  Native: not available ({e})")
            
    finally:
        cleanup_test_cache(cache_dir)


def benchmark_jpeg_decode(num_jpegs: int = 64, iterations: int = 3):
    """Benchmark JPEG decoding: native vs Python."""
    print(f"\n{'='*70}")
    print(f"JPEG DECODE BENCHMARK ({num_jpegs} images, {iterations} iterations)")
    print(f"{'='*70}")
    
    # Create test JPEGs in memory
    print("  Creating test JPEG data...")
    jpeg_data = create_realistic_jpeg(256, 256)
    jpeg_datas = [jpeg_data] * num_jpegs
    print(f"  JPEG size: {len(jpeg_data)} bytes x {num_jpegs} = {len(jpeg_data)*num_jpegs/1024:.0f} KB total")
    
    # Python baseline
    print("\n  Running Python decode...")
    python_times = []
    for i in range(iterations):
        start = time.perf_counter()
        results = python_decode_jpegs(jpeg_datas)
        python_times.append(time.perf_counter() - start)
    
    python_avg = sum(python_times) / len(python_times) * 1000
    decoded_count = sum(1 for r in results if r is not None)
    print(f"  Python: {python_avg:.2f}ms avg ({decoded_count}/{num_jpegs} decoded)")
    
    # Native (if available)
    try:
        from autoortho.aopipeline import AoDecode
        if AoDecode.is_available():
            print("  Running Native decode...")
            
            # Create buffer pool
            pool = AoDecode.create_pool(num_jpegs)
            
            # Warmup
            AoDecode.batch_decode(jpeg_datas[:4], pool)
            
            native_times = []
            for i in range(iterations):
                start = time.perf_counter()
                images = AoDecode.batch_decode(jpeg_datas, pool, max_threads=0)
                native_times.append(time.perf_counter() - start)
                # Free images
                AoDecode.free_images(images, pool)
            
            native_avg = sum(native_times) / len(native_times) * 1000
            speedup = python_avg / native_avg if native_avg > 0 else float('inf')
            
            pool.destroy()
            
            print(f"\n  Results:")
            print(f"    Python:          {python_avg:.2f}ms")
            print(f"    Native parallel: {native_avg:.2f}ms")
            print(f"    Speedup:         {speedup:.1f}x")
        else:
            print("  Native: not available")
    except ImportError as e:
        print(f"  Native: not available ({e})")


def benchmark_dds_build(chunks_per_side: int = 4, iterations: int = 3):
    """Benchmark full DDS building: native vs Python."""
    print(f"\n{'='*70}")
    print(f"FULL DDS BUILD BENCHMARK ({chunks_per_side}x{chunks_per_side} = {chunks_per_side**2} chunks)")
    print(f"{'='*70}")
    print(f"  This benchmarks the COMPLETE pipeline:")
    print(f"    1. Read cache files -> 2. Decode JPEGs -> 3. Compose tile")
    print(f"    4. Generate mipmaps -> 5. BC1/DXT1 compression")
    
    num_chunks = chunks_per_side * chunks_per_side
    cache_dir, paths = create_test_cache_dir(num_chunks, use_realistic=True)
    
    try:
        # Python baseline
        python_times = []
        python_result = None
        print("\n  Running Python full pipeline...")
        
        for i in range(iterations):
            start = time.perf_counter()
            python_result = python_build_dds_full(cache_dir, chunks_per_side)
            elapsed = time.perf_counter() - start
            if python_result is not None:
                python_times.append(elapsed)
        
        if python_times:
            python_avg = sum(python_times) / len(python_times) * 1000
            print(f"  Python: {python_avg:.2f}ms avg ({len(python_result) if python_result else 0} bytes)")
        else:
            python_avg = None
            print("  Python: failed or not available")
        
        # Native
        native_times = []
        native_result = None
        try:
            from autoortho.aopipeline import AoDDS
            if AoDDS.is_available():
                print("  Running Native full pipeline...")
                
                # Warmup
                try:
                    AoDDS.build_tile_native(
                        cache_dir=cache_dir,
                        row=0, col=0,
                        maptype="BI", zoom=16,
                        chunks_per_side=chunks_per_side,
                        format="BC1"
                    )
                except Exception:
                    pass
                
                for i in range(iterations):
                    start = time.perf_counter()
                    try:
                        native_result = AoDDS.build_tile_native_detailed(
                            cache_dir=cache_dir,
                            row=0, col=0,
                            maptype="BI", zoom=16,
                            chunks_per_side=chunks_per_side,
                            format="BC1"
                        )
                        native_times.append(time.perf_counter() - start)
                    except Exception as e:
                        print(f"    Build failed: {e}")
                
                if native_times and native_result and native_result.success:
                    native_avg = sum(native_times) / len(native_times) * 1000
                    
                    print(f"\n  Native Details:")
                    print(f"    Chunks decoded: {native_result.chunks_decoded}")
                    print(f"    Mipmaps:        {native_result.mipmaps}")
                    print(f"    Output size:    {len(native_result.data)} bytes")
                    print(f"    Internal time:  {native_result.elapsed_ms:.2f}ms")
                    
                    print(f"\n  Results:")
                    if python_avg:
                        print(f"    Python full pipeline:  {python_avg:.2f}ms")
                    print(f"    Native full pipeline:  {native_avg:.2f}ms")
                    
                    if python_avg:
                        speedup = python_avg / native_avg
                        print(f"    SPEEDUP:               {speedup:.1f}x")
                else:
                    print("  Native: build failed")
            else:
                print("  Native DDS: not available")
        except ImportError as e:
            print(f"  Native DDS: not available ({e})")
            
    finally:
        cleanup_test_cache(cache_dir)


def benchmark_dds_large_tile(iterations: int = 3):
    """Benchmark realistic 4096x4096 tile (16x16 chunks)."""
    print(f"\n{'='*70}")
    print("LARGE TILE BENCHMARK (16x16 = 256 chunks, 4096x4096 output)")
    print(f"{'='*70}")
    print("  This simulates a real AutoOrtho tile build")

    chunks_per_side = 16
    num_chunks = chunks_per_side * chunks_per_side
    cache_dir, paths = create_test_cache_dir(num_chunks, use_realistic=True)

    try:
        python_avg = None
        native_avg = None

        # Python pipeline (run once - it's slower)
        print("\n  Running Python full pipeline (16x16 tile)...")
        python_times = []
        for i in range(min(2, iterations)):  # Only 2 iterations, it's slow
            start = time.perf_counter()
            python_result = python_build_dds_full(cache_dir, chunks_per_side)
            elapsed = time.perf_counter() - start
            if python_result is not None:
                python_times.append(elapsed)

        if python_times:
            python_avg = sum(python_times) / len(python_times) * 1000
            py_size = len(python_result) if python_result else 0
            print(f"  Python: {python_avg:.2f}ms ({py_size/1024/1024:.2f} MB)")

        # Native pipeline
        try:
            from autoortho.aopipeline import AoDDS
            if AoDDS.is_available():
                print("  Running Native full pipeline (16x16 tile)...")

                # Warmup
                AoDDS.build_tile_native(
                    cache_dir=cache_dir,
                    row=0, col=0,
                    maptype="BI", zoom=16,
                    chunks_per_side=chunks_per_side,
                    format="BC1"
                )

                native_times = []
                for i in range(iterations):
                    start = time.perf_counter()
                    result = AoDDS.build_tile_native_detailed(
                        cache_dir=cache_dir,
                        row=0, col=0,
                        maptype="BI", zoom=16,
                        chunks_per_side=chunks_per_side,
                        format="BC1"
                    )
                    native_times.append(time.perf_counter() - start)

                native_avg = sum(native_times) / len(native_times) * 1000

                print(f"\n  Results (16x16 = 4096x4096 tile):")
                print(f"    Chunks decoded: {result.chunks_decoded}")
                print(f"    Mipmaps:        {result.mipmaps}")
                print(f"    Output size:    {len(result.data)/1024/1024:.2f} MB")
                print(f"    Internal time:  {result.elapsed_ms:.2f}ms")

                if python_avg:
                    print(f"\n  Comparison:")
                    print(f"    Python pipeline: {python_avg:.2f}ms")
                    print(f"    Native pipeline: {native_avg:.2f}ms")
                    speedup = python_avg / native_avg
                    print(f"    SPEEDUP:         {speedup:.1f}x")
                else:
                    print(f"\n    Native time:     {native_avg:.2f}ms")

                print(f"    Throughput:      {1000/native_avg:.1f} tiles/sec")
            else:
                print("  Native DDS: not available")
        except ImportError as e:
            print(f"  Native DDS: not available ({e})")

    finally:
        cleanup_test_cache(cache_dir)


def benchmark_hybrid_pipeline(chunks_per_side: int = 16, iterations: int = 3):
    """
    Benchmark the HYBRID approach: Python reads files, Native decodes+compresses.
    
    This is the optimal architecture that combines:
    - Python's fast cached file reads
    - Native's parallel decode and compression
    """
    print(f"\n{'='*70}")
    num_chunks = chunks_per_side * chunks_per_side
    print(f"HYBRID PIPELINE BENCHMARK ({chunks_per_side}x{chunks_per_side} = "
          f"{num_chunks} chunks)")
    print(f"{'='*70}")
    print("  Python reads files → Native decode+compress")
    print("  (Best of both worlds)")

    cache_dir, paths = create_test_cache_dir(num_chunks, use_realistic=True)

    try:
        python_avg = None
        native_avg = None
        hybrid_avg = None

        # 1. Pure Python pipeline
        print("\n  [1] Pure Python pipeline...")
        python_times = []
        for i in range(min(2, iterations)):
            start = time.perf_counter()
            result = python_build_dds_full(cache_dir, chunks_per_side)
            if result:
                python_times.append(time.perf_counter() - start)
        if python_times:
            python_avg = sum(python_times) / len(python_times) * 1000
            print(f"      Pure Python: {python_avg:.2f}ms")

        # 2. Native pipeline (file read + decode + compress in C)
        try:
            from autoortho.aopipeline import AoDDS
            if AoDDS.is_available():
                print("  [2] Native pipeline (C reads files)...")
                # Warmup
                AoDDS.build_tile_native(
                    cache_dir=cache_dir, row=0, col=0,
                    maptype="BI", zoom=16,
                    chunks_per_side=chunks_per_side, format="BC1"
                )
                native_times = []
                for i in range(iterations):
                    start = time.perf_counter()
                    result = AoDDS.build_tile_native_detailed(
                        cache_dir=cache_dir, row=0, col=0,
                        maptype="BI", zoom=16,
                        chunks_per_side=chunks_per_side, format="BC1"
                    )
                    native_times.append(time.perf_counter() - start)
                native_avg = sum(native_times) / len(native_times) * 1000
                print(f"      Native (C I/O): {native_avg:.2f}ms")
        except (ImportError, AttributeError) as e:
            print(f"      Native: not available ({e})")

        # 3. HYBRID: Python reads, Native decode+compress
        try:
            from autoortho.aopipeline import AoDDS
            if AoDDS.is_available() and hasattr(AoDDS, 'build_from_jpegs'):
                print("  [3] HYBRID pipeline (Python I/O, Native decode)...")

                # Warmup: Read files in Python
                jpeg_datas = []
                for path in paths:
                    try:
                        with open(path, 'rb') as f:
                            jpeg_datas.append(f.read())
                    except FileNotFoundError:
                        jpeg_datas.append(None)

                # Warmup native
                try:
                    AoDDS.build_from_jpegs(jpeg_datas)
                except Exception:
                    pass

                hybrid_times = []
                for i in range(iterations):
                    # Step 1: Python reads files (fast for cached)
                    start = time.perf_counter()
                    jpeg_datas = []
                    for path in paths:
                        try:
                            with open(path, 'rb') as f:
                                jpeg_datas.append(f.read())
                        except FileNotFoundError:
                            jpeg_datas.append(None)
                    read_time = time.perf_counter() - start

                    # Step 2: Native decode + compress
                    start2 = time.perf_counter()
                    _ = AoDDS.build_from_jpegs(jpeg_datas)
                    decode_time = time.perf_counter() - start2

                    total_time = read_time + decode_time
                    hybrid_times.append(total_time)

                hybrid_avg = sum(hybrid_times) / len(hybrid_times) * 1000
                last_read = read_time * 1000
                last_decode = decode_time * 1000
                print(f"      HYBRID:        {hybrid_avg:.2f}ms "
                      f"(read: ~{last_read:.1f}ms, "
                      f"decode+compress: ~{last_decode:.1f}ms)")
            else:
                print("      HYBRID: build_from_jpegs not available")
        except (ImportError, AttributeError, RuntimeError) as e:
            print(f"      HYBRID: not available ({e})")

        # Summary
        print(f"\n  Results ({chunks_per_side}x{chunks_per_side} tile):")
        if python_avg:
            print(f"    Pure Python:     {python_avg:.2f}ms")
        if native_avg:
            print(f"    Native (C I/O):  {native_avg:.2f}ms")
        if hybrid_avg:
            print(f"    HYBRID:          {hybrid_avg:.2f}ms")

        if python_avg and hybrid_avg:
            speedup = python_avg / hybrid_avg
            print(f"\n    HYBRID vs Python: {speedup:.1f}x faster")
        if native_avg and hybrid_avg:
            speedup = native_avg / hybrid_avg
            print(f"    HYBRID vs Native: {speedup:.1f}x faster")
            
    finally:
        cleanup_test_cache(cache_dir)


def benchmark_bundle_format(chunks_per_side: int = 16, iterations: int = 3):
    """
    Benchmark the BUNDLE format: single consolidated file.
    
    This tests the optimal I/O pattern: 256 files → 1 bundle file.
    """
    print(f"\n{'='*70}")
    num_chunks = chunks_per_side * chunks_per_side
    print(f"BUNDLE FORMAT BENCHMARK ({chunks_per_side}x{chunks_per_side} = "
          f"{num_chunks} chunks)")
    print(f"{'='*70}")
    print("  256 files → 1 bundle file")
    print("  Single file open + mmap = minimal I/O overhead")

    cache_dir, paths = create_test_cache_dir(num_chunks, use_realistic=True)
    bundle_path = os.path.join(cache_dir, f"test_bundle_{num_chunks}.aob")

    try:
        individual_avg = None
        bundle_create_time = None
        bundle_read_avg = None
        bundle_dds_avg = None

        # 1. Measure individual file reads
        print("\n  [1] Individual file reads (256 files)...")
        individual_times = []
        for i in range(iterations):
            start = time.perf_counter()
            for path in paths:
                try:
                    with open(path, 'rb') as f:
                        _ = f.read()
                except FileNotFoundError:
                    pass
            individual_times.append(time.perf_counter() - start)
        individual_avg = sum(individual_times) / len(individual_times) * 1000
        print(f"      Individual reads: {individual_avg:.2f}ms")

        # 2. Create bundle (measure once)
        try:
            from autoortho.aopipeline import AoBundle
            if AoBundle.is_available():
                print("  [2] Creating bundle file...")
                start = time.perf_counter()
                AoBundle.create_bundle(
                    cache_dir=cache_dir,
                    tile_col=0, tile_row=0,
                    maptype="BI", zoom=16,
                    chunks_per_side=chunks_per_side,
                    output_path=bundle_path
                )
                bundle_create_time = (time.perf_counter() - start) * 1000

                bundle_size = os.path.getsize(bundle_path) if os.path.exists(bundle_path) else 0
                print(f"      Bundle created: {bundle_create_time:.2f}ms "
                      f"({bundle_size/1024:.1f} KB)")

                # 3. Read entire bundle (compare to individual reads)
                print("  [3] Reading bundle file...")
                bundle_times = []
                for i in range(iterations):
                    start = time.perf_counter()
                    with open(bundle_path, 'rb') as f:
                        _ = f.read()
                    bundle_times.append(time.perf_counter() - start)
                bundle_read_avg = sum(bundle_times) / len(bundle_times) * 1000
                print(f"      Bundle read:     {bundle_read_avg:.2f}ms")

                # 4. Full DDS from bundle (if available)
                print("  [4] DDS from bundle...")
                try:
                    # Warmup
                    dds = AoBundle.build_dds_from_bundle(bundle_path)
                    
                    bundle_dds_times = []
                    for i in range(iterations):
                        start = time.perf_counter()
                        dds = AoBundle.build_dds_from_bundle(bundle_path)
                        bundle_dds_times.append(time.perf_counter() - start)
                    bundle_dds_avg = sum(bundle_dds_times) / len(bundle_dds_times) * 1000
                    print(f"      Bundle → DDS:    {bundle_dds_avg:.2f}ms "
                          f"({len(dds)/1024/1024:.2f} MB)")
                except Exception as e:
                    print(f"      Bundle → DDS:    not available ({e})")
            else:
                print("      Bundle: native not available, using Python fallback")
                start = time.perf_counter()
                AoBundle.create_bundle_python(
                    cache_dir=cache_dir,
                    tile_col=0, tile_row=0,
                    maptype="BI", zoom=16,
                    chunks_per_side=chunks_per_side,
                    output_path=bundle_path
                )
                bundle_create_time = (time.perf_counter() - start) * 1000
                bundle_size = os.path.getsize(bundle_path) if os.path.exists(bundle_path) else 0
                print(f"      Bundle created (Python): {bundle_create_time:.2f}ms "
                      f"({bundle_size/1024:.1f} KB)")

        except ImportError as e:
            print(f"      Bundle: not available ({e})")

        # Summary
        print(f"\n  I/O Comparison:")
        if individual_avg:
            print(f"    256 individual files: {individual_avg:.2f}ms")
        if bundle_read_avg:
            print(f"    1 bundle file:        {bundle_read_avg:.2f}ms")
            if individual_avg:
                speedup = individual_avg / bundle_read_avg
                print(f"    I/O SPEEDUP:          {speedup:.1f}x faster")

        if bundle_dds_avg:
            print(f"\n  Complete Tile Build (I/O + Decode + Compress):")
            print(f"    Bundle → DDS:         {bundle_dds_avg:.2f}ms")

    finally:
        # Clean up bundle file
        if os.path.exists(bundle_path):
            os.remove(bundle_path)
        cleanup_test_cache(cache_dir)


def benchmark_end_to_end(chunks_per_side: int = 16, iterations: int = 3):
    """
    END-TO-END benchmark: Tile Creation → Cache → DDS Build → Serve
    
    Simulates the complete AutoOrtho workflow:
    1. Create mock tile with chunks (simulates tile request)
    2. Load chunk data from cache (simulates cache hits)
    3. Build complete DDS (decode + compress)
    4. Return DDS bytes (simulates serving to X-Plane)
    
    Tests three approaches:
    - Pure Python: Python reads → Python decode → Python compress
    - Legacy Native: C reads files → C decode → C compress
    - Hybrid (Optimal): Python reads → C decode → C compress
    """
    print(f"\n{'='*70}")
    num_chunks = chunks_per_side * chunks_per_side
    print(f"END-TO-END PIPELINE BENCHMARK ({chunks_per_side}x{chunks_per_side} = "
          f"{num_chunks} chunks)")
    print(f"{'='*70}")
    print("  Simulates complete AutoOrtho tile workflow:")
    print("  Tile Creation → Cache Load → DDS Build → Serve")
    print()
    
    cache_dir, paths = create_test_cache_dir(num_chunks, use_realistic=True)
    
    try:
        # Get format settings
        dxt_format = "BC1"
        missing_color = (66, 77, 55)
        
        # =====================================================================
        # APPROACH 1: Pure Python Pipeline
        # =====================================================================
        print("  [1] PURE PYTHON PIPELINE")
        print("      Python reads → Python decode (AoImage) → Python compress")
        
        python_times = []
        python_dds_size = 0
        
        # Use the existing python_build_dds_full which handles all the imports
        for i in range(iterations):
            start = time.perf_counter()
            result = python_build_dds_full(cache_dir, chunks_per_side)
            total_time = time.perf_counter() - start
            
            if result:
                python_times.append(total_time)
                python_dds_size = len(result)
        
        if python_times:
            python_avg = sum(python_times) / len(python_times) * 1000
            print(f"      TOTAL: {python_avg:.2f}ms avg "
                  f"({python_dds_size/1024/1024:.2f} MB)")
        else:
            python_avg = None
            print("      Failed to build (missing dependencies)")
        
        # =====================================================================
        # APPROACH 2: Legacy Native Pipeline (C reads files)
        # =====================================================================
        print("\n  [2] LEGACY NATIVE PIPELINE (C reads files)")
        print("      C reads files → C decode → C compress")
        
        native_times = []
        native_dds_size = 0
        
        try:
            from autoortho.aopipeline import AoDDS
            if AoDDS.is_available():
                # Warmup
                try:
                    AoDDS.build_tile_native(
                        cache_dir=cache_dir, row=0, col=0,
                        maptype="BI", zoom=16,
                        chunks_per_side=chunks_per_side, format=dxt_format
                    )
                except Exception:
                    pass
                
                for i in range(iterations):
                    start = time.perf_counter()
                    
                    result = AoDDS.build_tile_native_detailed(
                        cache_dir=cache_dir, row=0, col=0,
                        maptype="BI", zoom=16,
                        chunks_per_side=chunks_per_side,
                        format=dxt_format,
                        missing_color=missing_color
                    )
                    
                    total_time = time.perf_counter() - start
                    native_times.append(total_time)
                    native_dds_size = len(result.data) if result.success else 0
                    
                    if i == 0:
                        print(f"      Internal: {result.elapsed_ms:.1f}ms | "
                              f"Chunks: {result.chunks_decoded}")
                
                native_avg = sum(native_times) / len(native_times) * 1000
                print(f"      TOTAL: {native_avg:.2f}ms avg "
                      f"({native_dds_size/1024/1024:.2f} MB)")
            else:
                print("      Not available")
                native_avg = None
        except ImportError:
            print("      Not available")
            native_avg = None
        
        # =====================================================================
        # APPROACH 3: HYBRID Pipeline (Optimal)
        # =====================================================================
        print("\n  [3] HYBRID PIPELINE (OPTIMAL)")
        print("      Python reads → C decode → C compress")
        print("      (Uses chunk.data directly, no file I/O in C)")
        
        hybrid_times = []
        hybrid_dds_size = 0
        hybrid_breakdown = {}
        
        try:
            from autoortho.aopipeline import AoDDS
            if AoDDS.is_available() and hasattr(AoDDS, 'build_from_jpegs'):
                # Warmup
                jpeg_datas = []
                for path in paths:
                    try:
                        with open(path, 'rb') as f:
                            jpeg_datas.append(f.read())
                    except FileNotFoundError:
                        jpeg_datas.append(None)
                try:
                    AoDDS.build_from_jpegs(jpeg_datas)
                except Exception:
                    pass
                
                for i in range(iterations):
                    start = time.perf_counter()
                    
                    # Step 1: Python reads files (fast for cached)
                    jpeg_datas = []
                    for path in paths:
                        try:
                            with open(path, 'rb') as f:
                                jpeg_datas.append(f.read())
                        except FileNotFoundError:
                            jpeg_datas.append(None)
                    read_time = time.perf_counter() - start
                    
                    # Step 2: Native decode + compress
                    native_start = time.perf_counter()
                    dds_bytes = AoDDS.build_from_jpegs(
                        jpeg_datas,
                        format=dxt_format,
                        missing_color=missing_color
                    )
                    native_time = time.perf_counter() - native_start
                    
                    total_time = time.perf_counter() - start
                    hybrid_times.append(total_time)
                    hybrid_dds_size = len(dds_bytes) if dds_bytes else 0
                    
                    if i == 0:
                        hybrid_breakdown = {
                            'read': read_time * 1000,
                            'native': native_time * 1000
                        }
                        print(f"      Read: {read_time*1000:.1f}ms | "
                              f"Native: {native_time*1000:.1f}ms")
                
                hybrid_avg = sum(hybrid_times) / len(hybrid_times) * 1000
                print(f"      TOTAL: {hybrid_avg:.2f}ms avg "
                      f"({hybrid_dds_size/1024/1024:.2f} MB)")
            else:
                print("      build_from_jpegs not available")
                hybrid_avg = None
        except ImportError:
            print("      Not available")
            hybrid_avg = None
        
        # =====================================================================
        # SUMMARY
        # =====================================================================
        print(f"\n  {'─'*66}")
        print("  END-TO-END SUMMARY")
        print(f"  {'─'*66}")
        print(f"  Tile: {chunks_per_side}x{chunks_per_side} = {num_chunks} chunks "
              f"({chunks_per_side*256}x{chunks_per_side*256} pixels)")
        print()
        
        results = []
        if python_avg:
            results.append(("Pure Python", python_avg, python_dds_size))
        if native_avg:
            results.append(("Legacy Native", native_avg, native_dds_size))
        if hybrid_avg:
            results.append(("HYBRID (Optimal)", hybrid_avg, hybrid_dds_size))
        
        # Sort by time (only include valid results)
        results = [r for r in results if r[1] is not None]
        if not results:
            print("  No valid results!")
            return
            
        results.sort(key=lambda x: x[1])
        
        print("  Approach             Time       Speedup   Throughput")
        print("  " + "─" * 55)
        
        # Use Python as baseline if available, otherwise use slowest
        baseline = python_avg if python_avg else max(r[1] for r in results)
        for name, time_ms, size in results:
            speedup = baseline / time_ms
            throughput = 1000 / time_ms
            marker = " ← FASTEST" if time_ms == results[0][1] else ""
            print(f"  {name:20} {time_ms:7.2f}ms   {speedup:5.2f}x    "
                  f"{throughput:5.1f} tiles/sec{marker}")
        
        print()
        if hybrid_avg and python_avg:
            savings = python_avg - hybrid_avg
            savings_pct = (savings / python_avg) * 100
            print(f"  Time saved per tile: {savings:.1f}ms ({savings_pct:.1f}%)")
            
            # Estimate impact for typical flight
            tiles_per_minute = 60000 / hybrid_avg  # How many tiles we can build
            print(f"  Tiles buildable per minute: {tiles_per_minute:.0f}")
        
        if hybrid_breakdown:
            print(f"\n  Hybrid breakdown:")
            print(f"    File I/O (Python):     {hybrid_breakdown['read']:.1f}ms "
                  f"({hybrid_breakdown['read']/hybrid_avg*100:.0f}%)")
            print(f"    Decode+Compress (C):   {hybrid_breakdown['native']:.1f}ms "
                  f"({hybrid_breakdown['native']/hybrid_avg*100:.0f}%)")
            
    finally:
        cleanup_test_cache(cache_dir)


def benchmark_component_availability():
    """Check which native components are available."""
    print(f"\n{'='*70}")
    print("COMPONENT AVAILABILITY")
    print(f"{'='*70}")
    
    components = [
        ("AoCache", "autoortho.aopipeline.AoCache"),
        ("AoDecode", "autoortho.aopipeline.AoDecode"),
        ("AoDDS", "autoortho.aopipeline.AoDDS"),
    ]
    
    all_available = True
    for name, module_path in components:
        try:
            parts = module_path.rsplit('.', 1)
            mod = __import__(parts[0], fromlist=[parts[1]])
            component = getattr(mod, parts[1])
            if hasattr(component, 'is_available'):
                available = component.is_available()
            elif hasattr(component, 'get_version'):
                available = True
            else:
                available = True
            
            if available:
                if hasattr(component, 'get_version'):
                    version = component.get_version()
                else:
                    version = "available"
                print(f"  {name}: ✓ {version}")
            else:
                print(f"  {name}: ✗ library not found")
                all_available = False
        except ImportError as e:
            print(f"  {name}: ✗ import failed ({e})")
            all_available = False
        except Exception as e:
            print(f"  {name}: ✗ error ({e})")
            all_available = False
    
    # Check Python components
    print("\n  Python components:")
    try:
        from autoortho.aoimage.AoImage import AoImage
        print(f"  AoImage: ✓ available")
    except ImportError as e:
        print(f"  AoImage: ✗ ({e})")
    
    try:
        from autoortho.pydds import DDS
        print(f"  PyDDS:   ✓ available")
    except ImportError as e:
        print(f"  PyDDS:   ✗ ({e})")
    
    return all_available


def benchmark_buffer_pool(chunks_per_side: int = 16, iterations: int = 5):
    """
    Benchmark buffer pool vs standard allocation for DDS building.
    
    This directly measures the benefit of pre-allocated numpy buffers:
    - No allocation overhead (~15ms saved)
    - No copy overhead when using memoryview (~65ms saved)
    """
    print(f"\n{'='*70}")
    print(f"BUFFER POOL BENCHMARK ({chunks_per_side}x{chunks_per_side} = {chunks_per_side**2} chunks)")
    print(f"{'='*70}")
    print("  Pre-allocated numpy buffers vs per-call ctypes allocation")
    
    num_chunks = chunks_per_side * chunks_per_side
    cache_dir, paths = create_test_cache_dir(num_chunks, use_realistic=True)
    
    try:
        from autoortho.aopipeline import AoDDS
        if not AoDDS.is_available():
            print("  Native DDS: not available")
            return
        
        if not hasattr(AoDDS, 'DDSBufferPool') or not hasattr(AoDDS, 'build_from_jpegs_to_buffer'):
            print("  Buffer pool: not available (rebuild native library)")
            return
        
        # Read all JPEGs into memory (simulating the hybrid approach)
        jpeg_datas = []
        for path in paths:
            with open(path, 'rb') as f:
                jpeg_datas.append(f.read())
        
        print(f"  JPEG data size: {sum(len(d) for d in jpeg_datas) / 1024:.1f} KB")
        
        # Warmup
        for _ in range(2):
            try:
                AoDDS.build_from_jpegs(jpeg_datas, format="BC1")
            except Exception:
                pass
        
        # Benchmark 1: Standard allocation (per-call ctypes buffer)
        print("\n  [1] Standard allocation (ctypes buffer per call)...")
        standard_times = []
        for _ in range(iterations):
            start = time.perf_counter()
            result = AoDDS.build_from_jpegs(jpeg_datas, format="BC1")
            elapsed = time.perf_counter() - start
            standard_times.append(elapsed)
        
        standard_avg = sum(standard_times) / len(standard_times) * 1000
        print(f"      Standard: {standard_avg:.2f}ms avg ({len(result)} bytes)")
        
        # Benchmark 2: Buffer pool (pre-allocated numpy buffer)
        print("\n  [2] Buffer pool (pre-allocated numpy, zero-copy build)...")
        pool = AoDDS.DDSBufferPool(
            buffer_size=AoDDS.DDSBufferPool.SIZE_4096x4096_BC1,
            pool_size=2
        )
        
        pool_times = []
        pool_result = None
        for _ in range(iterations):
            buffer, buffer_id = pool.acquire()
            try:
                start = time.perf_counter()
                pool_result = AoDDS.build_from_jpegs_to_buffer(
                    buffer, jpeg_datas, format="BC1"
                )
                elapsed = time.perf_counter() - start
                if pool_result.success:
                    pool_times.append(elapsed)
            finally:
                pool.release(buffer_id)
        
        if pool_times:
            pool_avg = sum(pool_times) / len(pool_times) * 1000
            print(f"      Buffer pool: {pool_avg:.2f}ms avg ({pool_result.bytes_written} bytes)")
        else:
            print(f"      Buffer pool: FAILED - {pool_result.error if pool_result else 'No result'}")
            return
        
        # Benchmark 3: Buffer pool with to_bytes() (measures copy overhead)
        print("\n  [3] Buffer pool with to_bytes() (includes copy)...")
        pool_copy_times = []
        dds_bytes = b''
        for _ in range(iterations):
            buffer, buffer_id = pool.acquire()
            try:
                start = time.perf_counter()
                copy_result = AoDDS.build_from_jpegs_to_buffer(
                    buffer, jpeg_datas, format="BC1"
                )
                if copy_result.success:
                    dds_bytes = copy_result.to_bytes()  # Force copy
                    elapsed = time.perf_counter() - start
                    pool_copy_times.append(elapsed)
            finally:
                pool.release(buffer_id)
        
        if pool_copy_times:
            pool_copy_avg = sum(pool_copy_times) / len(pool_copy_times) * 1000
            print(f"      Pool + copy: {pool_copy_avg:.2f}ms avg ({len(dds_bytes)} bytes)")
        else:
            print(f"      Pool + copy: FAILED")
            return
        
        # Results summary
        print(f"\n  Results:")
        print(f"    Standard (alloc+build+copy):  {standard_avg:.2f}ms")
        print(f"    Pool (build only):            {pool_avg:.2f}ms")
        print(f"    Pool + copy:                  {pool_copy_avg:.2f}ms")
        print()
        print(f"    Allocation overhead:          {standard_avg - pool_copy_avg:.2f}ms")
        print(f"    Copy overhead:                {pool_copy_avg - pool_avg:.2f}ms")
        print(f"    Total savings (pool only):    {standard_avg - pool_avg:.2f}ms ({(1 - pool_avg/standard_avg)*100:.1f}%)")
        
        if standard_avg > pool_avg:
            speedup = standard_avg / pool_avg
            print(f"    Speedup:                      {speedup:.2f}x")
        
    except ImportError as e:
        print(f"  Native DDS: not available ({e})")
    except Exception as e:
        print(f"  Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        cleanup_test_cache(cache_dir)


def run_all_benchmarks(quick: bool = False):
    """Run all benchmarks."""
    import multiprocessing
    
    print("\n" + "="*70)
    print("AUTOORTHO NATIVE PIPELINE BENCHMARKS")
    print("="*70)
    print(f"  System: {sys.platform}")
    print(f"  CPU cores: {multiprocessing.cpu_count()}")
    print(f"  Python: {sys.version.split()[0]}")
    
    all_available = benchmark_component_availability()
    
    if quick:
        print("\n  Running QUICK benchmarks...")
        benchmark_cache_read(num_files=64, iterations=3)
        benchmark_jpeg_decode(num_jpegs=16, iterations=2)
        benchmark_dds_build(chunks_per_side=4, iterations=2)
        benchmark_hybrid_pipeline(chunks_per_side=4, iterations=2)
        benchmark_buffer_pool(chunks_per_side=4, iterations=3)
        benchmark_bundle_format(chunks_per_side=4, iterations=2)
        benchmark_end_to_end(chunks_per_side=4, iterations=2)
    else:
        print("\n  Running FULL benchmarks...")
        benchmark_cache_read(num_files=256, iterations=5)
        benchmark_jpeg_decode(num_jpegs=64, iterations=3)
        benchmark_dds_build(chunks_per_side=4, iterations=3)
        benchmark_dds_build(chunks_per_side=8, iterations=3)  # Medium tile

        if all_available:
            benchmark_dds_large_tile(iterations=3)
            benchmark_hybrid_pipeline(chunks_per_side=16, iterations=3)
            benchmark_buffer_pool(chunks_per_side=16, iterations=5)
            benchmark_bundle_format(chunks_per_side=16, iterations=3)
            benchmark_end_to_end(chunks_per_side=16, iterations=3)
    
    print(f"\n{'='*70}")
    print("BENCHMARK COMPLETE")
    print(f"{'='*70}")

    if all_available:
        print("\n  Summary:")
        print("  The HYBRID approach (Python I/O + Native decode) is optimal:")
        print("    - Python reads cached files (OS cache = microsecond latency)")
        print("    - Native does parallel JPEG decode (turbojpeg + OpenMP)")
        print("    - Native does ISPC vectorized DXT compression")
        print("    - Single ctypes call minimizes boundary overhead")
        print()
        print("  Key insight: Native file I/O is slower than Python for cached")
        print("  files due to OpenMP thread overhead. But native decode+compress")
        print("  is 3x faster due to true parallelism.")
    print()


# ============================================================================
# Main
# ============================================================================

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='Benchmark native vs Python pipeline')
    parser.add_argument('--quick', '-q', action='store_true', 
                        help='Run quick benchmarks with smaller data')
    args = parser.parse_args()
    
    run_all_benchmarks(quick=args.quick)

